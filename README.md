# Study Pal - AI Study Assistant

An intelligent document assistant powered by RAG (Retrieval-Augmented Generation) technology that remembers your learning preferences and background to provide personalized study support.

**Powered by:** LangChain â€¢ Google Gemini â€¢ Ollama â€¢ Streamlit

---

## âœ¨ Core Features

- **ğŸ“„ PDF Document Analysis**: Smart PDF parsing with content-based Q&A and source citations
- **ğŸ¤– Dual Model Support**: Switch freely between Google Gemini API and local Ollama models
- **ğŸ§  Personalized Memory**: AI remembers your learning style, background, and preferences for customized responses
- **ğŸ¨ Modern Interface**: Streamlit web UI with split-screen PDF preview and chat
- **ğŸ’¾ Privacy-First**: Memory data stored locally in SQLite for maximum privacy

## ğŸš€ Quick Start

### One-Line Installation (Recommended)

```bash
# 1. Create environment
conda create -n rag_system python=3.10 -y
conda activate rag_system

# 2. Install Python dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 3. Install Ollama (for local models, optional)
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull phi3:mini

# 4. Configure API keys
python setup_config.py

# 5. Launch application
python run_gui.py
```

### Configuration Guide

Run `python setup_config.py` and follow the prompts:
- **Google API Key**: Get from [Google AI Studio](https://makersuite.google.com/app/apikey)
- **Default Models**: Choose Google or Ollama models
- **Memory System**: Enable personalized memory features

Config file location: `~/.config/llm_project/.env`

---

## ğŸ’¡ User Guide

### Web Interface (Recommended)

```bash
python run_gui.py
# Access http://localhost:8501 (WSL users see WSL configuration below)
```

**Interface Features:**
- ğŸ“¤ Upload PDF documents
- ğŸ’¬ Chat with AI to analyze documents
- ğŸ”„ Switch between Google/Ollama models
- ğŸ§  Manage personalized memory

### Personalized Memory System

Let AI remember your learning preferences for customized assistance:

**How to Use:**
1. Chat with AI and tell it about your background (e.g., "I'm a beginner, need simple explanations")
2. Click **"ğŸ§© Generate & Merge"** in the sidebar
3. Refresh the page to start a new session
4. Upload a PDF and ask questions - AI will provide personalized responses based on your memory!

**Example Scenarios:**

| Scenario | Memory Settings | AI Response Style |
|----------|----------------|-------------------|
| ğŸ“ Beginner | "I'm a programming beginner, need simple explanations" | Uses analogies, avoids jargon, step-by-step approach |
| ğŸ”¬ Researcher | "I'm an ML researcher, need technical depth and citations" | Provides technical details, math formulas, research references |
| ğŸ“ Exam Prep | "I'm preparing for exams, focus on formulas and key concepts" | Structured summaries, key formulas, concept highlights |

**Memory Controls:**
- ğŸ§© Generate & Merge: Create/update memory from conversation
- ğŸ—‘ï¸ Clear Memory: Delete all saved memory
- ğŸ”„ New Session: Reset chat and reload memory
- View Memory: Run `python print_memory.py`

---

## ğŸ“ Project Structure

```
LLM_project/
â”œâ”€â”€ rag_modules/           # Core RAG modules
â”‚   â”œâ”€â”€ providers/         # Model providers (Google, Ollama)
â”‚   â”œâ”€â”€ utils/             # Utility functions (PDF processing, etc.)
â”‚   â””â”€â”€ core/              # RAG chain builder
â”œâ”€â”€ memory/                # Memory system
â”‚   â”œâ”€â”€ generator.py       # Memory generation
â”‚   â””â”€â”€ rolling.py         # Rolling memory manager
â”œâ”€â”€ data/                  # Data storage
â”‚   â””â”€â”€ memory.db          # SQLite database
â”œâ”€â”€ streamlit_app.py       # Web application
â”œâ”€â”€ run_gui.py             # GUI launcher (WSL optimized)
â”œâ”€â”€ setup_config.py        # Configuration wizard
â””â”€â”€ requirements.txt       # Python dependencies
```

---

## ğŸ› ï¸ System Requirements

| Component | Requirement |
|-----------|-------------|
| Python | 3.9 - 3.11 |
| RAM | Minimum 8GB (16GB+ recommended) |
| OS | Linux / macOS / Windows (WSL2) |
| Disk Space | ~5GB (dependencies + models) |

---

## ğŸ”§ Advanced Configuration

### Model Selection

**Google Models:**
```bash
# Available models: gemini-1.5-pro, gemini-1.5-flash
# Modify in .env file or switch via GUI interface
```

**Local Ollama Models:**
```bash
ollama list                    # List installed models
ollama pull mistral:7b         # Download additional models
ollama pull codellama:7b       # Code-specialized model
```

### GPU Acceleration (Optional)

```bash
# Check CUDA availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# Install PyTorch with CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### WSL Network Configuration

If running in WSL and Windows browser cannot access `localhost:8501`:

```bash
# Option 1: Use optimized launcher
python run_gui.py

# Option 2: Check WSL IP
ip addr show eth0
# Access from Windows browser: http://[WSL_IP]:8501
```

---

## ğŸ› Common Issues

<details>
<summary><b>Ollama Connection Failed</b></summary>

```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```
</details>

<details>
<summary><b>Import Errors</b></summary>

```bash
# Ensure environment is activated
conda activate rag_system

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```
</details>

<details>
<summary><b>Memory Not Saving</b></summary>

```bash
# Check database file
ls -la data/memory.db

# View memory content
python print_memory.py

# Reset database (deletes all memories!)
rm data/memory.db
```
</details>

<details>
<summary><b>Google API Errors</b></summary>

```bash
# Verify API key configuration
cat ~/.config/llm_project/.env | grep GOOGLE_API_KEY

# Reconfigure
python setup_config.py
```
</details>

---

## ğŸ“Š Testing Tools

```bash
python test_models.py              # Test model connections
python test_memory_integration.py  # Test memory system
python print_memory.py             # View memory content
```

---

## ğŸ“š Documentation

- [LangChain Documentation](https://python.langchain.com/)
- [Ollama Model Library](https://ollama.ai/library)
- [Google Gemini API](https://ai.google.dev/)
- [Streamlit Documentation](https://docs.streamlit.io/)

---

## ğŸ” Security Notes

- âš ï¸ Never commit API keys to the repository
- ğŸ”’ Keys are stored in local `.env` file (ignored by version control)
- ğŸ›¡ï¸ Memory data is stored locally only, never uploaded to the cloud

---

**Version:** v1.0  
**License:** MIT  
**Author:** Study Pal Team